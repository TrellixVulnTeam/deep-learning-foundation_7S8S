{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 4:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1003, 1: 963, 2: 1041, 3: 976, 4: 1004, 5: 1021, 6: 1004, 7: 981, 8: 1024, 9: 983}\n",
      "First 20 Labels: [0, 6, 0, 2, 7, 2, 1, 2, 4, 1, 5, 6, 6, 3, 1, 3, 5, 5, 8, 1]\n",
      "\n",
      "Example of Image 2:\n",
      "Image - Min Value: 2 Max Value: 204\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 0 Name: airplane\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noDFBDwCNCXoAaEzQA0Bjgh4AGvvfpbpYV+6ysa0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa1aa08fe10>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 4\n",
    "sample_id = 2\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # return None\n",
    "    a = 0\n",
    "    b = 255\n",
    "    return (x-a)/(b-a)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # return None\n",
    "    y = np.zeros((len(x), 10))\n",
    "    for i in range(len(x)):\n",
    "        y[i,x[i]] = 1\n",
    "    return y\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Network Parameters\n",
    "#n_input = 3072 # MNIST data input (img shape: 28*28)\n",
    "#n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # return None\n",
    "    #print(image_shape)    \n",
    "    # return tf.placeholder(tf.float32, shape=(32, 32, 3))\n",
    "    return tf.placeholder(tf.float32, shape=[None, image_shape[0], image_shape[1], image_shape[2]], name='x')\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #print(n_classes)\n",
    "    return tf.placeholder(tf.float32, [None, n_classes], name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, None, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #print(x_tensor)\n",
    "    input_channel_depth = int(x_tensor.get_shape()[3])\n",
    "    # The shape of the filter weight is (height, width, input_depth, output_depth)\n",
    "    filter_weights = tf.Variable(tf.truncated_normal([*conv_ksize, input_channel_depth, conv_num_outputs], dtype=tf.float32))\n",
    "    # The shape of the biases is equal the the number of outputs of the conv layer\n",
    "    filter_biases = tf.Variable(tf.constant(0, shape=[conv_num_outputs], dtype=tf.float32))\n",
    "    \n",
    "    layer = tf.nn.conv2d(input=x_tensor, filter=filter_weights, strides=[1, *conv_strides, 1], padding='SAME')\n",
    "    layer += filter_biases\n",
    "    layer = tf.nn.max_pool(layer, [1, *pool_ksize, 1], strides=[1, *pool_strides, 1], padding='SAME')\n",
    "    return layer \n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from numpy import prod\n",
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    batch_size = x_tensor.get_shape()[0]\n",
    "    #flattened_image_size = x_tensor.get_shape()[1]*x_tensor.get_shape()[2]*x_tensor.get_shape()[3]\n",
    "    #print(flattened_image_size)\n",
    "    #return tf.contrib.layers.flatten(x_tensor,[batch_size,flattened_image_size])\n",
    "    #return tf.reshape(x_tensor, [batch_size,flattened_image_size])\n",
    "    dimension = x_tensor.get_shape().as_list()    \n",
    "    #print(prod(dimension[1:]))\n",
    "    return tf.reshape(x_tensor,[-1,prod(dimension[1:])])\n",
    "   \n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # return None\n",
    "    #print(x_tensor)\n",
    "    #print(num_outputs)\n",
    "    shape = x_tensor.get_shape().as_list()[1:]\n",
    "    shape.append(num_outputs)\n",
    "    #print(shape)\n",
    "    weight = tf.Variable(tf.truncated_normal(shape,0,0.1))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    return tf.nn.relu(tf.add(tf.matmul(x_tensor,weight), bias))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #return None\n",
    "    shape = x_tensor.get_shape().as_list()[1:]\n",
    "    shape.append(num_outputs)\n",
    "    #print(shape)\n",
    "    weight = tf.Variable(tf.truncated_normal(shape,0,0.01))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    return tf.nn.relu(tf.add(tf.matmul(x_tensor,weight), bias))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    #print(x)\n",
    "    x_tensor = x\n",
    "    conv_num_outputs = 10\n",
    "    conv_ksize = (4, 4)\n",
    "    conv_strides = (1, 1)\n",
    "    pool_ksize = (8, 8)\n",
    "    pool_strides = (1, 1)\n",
    "    \n",
    "    model = conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    # Apply some dropout\n",
    "    model = tf.nn.dropout(model, keep_prob)\n",
    "    \n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    model = flatten(model)\n",
    "    \n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    num_outputs = 10 \n",
    "    model = fully_conn(model, num_outputs)\n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    model = output(model, num_outputs)\n",
    "    \n",
    "    \n",
    "    # TODO: return output\n",
    "    return model\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={x:feature_batch, y:label_batch, keep_prob:keep_probability})\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #pass\n",
    "    # Loss and Optimizer\n",
    "    #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    #optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "    # Accuracy\n",
    "    #correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    #accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "    \n",
    "    #print(cost, accuracy)\n",
    "    loss = session.run(cost, feed_dict={x:feature_batch, y:label_batch, keep_prob:1.0})\n",
    "    valid_acc = sess.run(accuracy, feed_dict={\n",
    "                x: valid_features,\n",
    "                y: valid_labels,\n",
    "                keep_prob: 1.})\n",
    "    print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(\n",
    "                loss,\n",
    "                valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 200\n",
    "batch_size = 256\n",
    "keep_probability = 0.60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.1120 Validation Accuracy: 0.292400\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.0712 Validation Accuracy: 0.340400\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     2.0060 Validation Accuracy: 0.346000\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.9459 Validation Accuracy: 0.360200\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.8902 Validation Accuracy: 0.370400\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.8147 Validation Accuracy: 0.376800\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.7604 Validation Accuracy: 0.380600\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.7066 Validation Accuracy: 0.390600\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.6632 Validation Accuracy: 0.394600\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.6340 Validation Accuracy: 0.398200\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.5975 Validation Accuracy: 0.404200\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.5644 Validation Accuracy: 0.407600\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.5570 Validation Accuracy: 0.404800\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.5324 Validation Accuracy: 0.410800\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.4754 Validation Accuracy: 0.409400\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.4416 Validation Accuracy: 0.412600\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.4140 Validation Accuracy: 0.410800\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.3732 Validation Accuracy: 0.417800\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.3437 Validation Accuracy: 0.414800\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.3291 Validation Accuracy: 0.418400\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.3096 Validation Accuracy: 0.417800\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     1.3252 Validation Accuracy: 0.413400\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     1.3005 Validation Accuracy: 0.420200\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     1.2815 Validation Accuracy: 0.424800\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     1.2559 Validation Accuracy: 0.430800\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     1.2506 Validation Accuracy: 0.427800\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     1.2345 Validation Accuracy: 0.427000\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     1.2193 Validation Accuracy: 0.426800\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     1.2004 Validation Accuracy: 0.426800\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     1.1912 Validation Accuracy: 0.427400\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     1.1702 Validation Accuracy: 0.429400\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     1.1839 Validation Accuracy: 0.432400\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     1.1607 Validation Accuracy: 0.432600\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     1.1652 Validation Accuracy: 0.429400\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     1.1300 Validation Accuracy: 0.437600\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     1.1255 Validation Accuracy: 0.437600\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     1.1284 Validation Accuracy: 0.430800\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     1.0984 Validation Accuracy: 0.432600\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     1.0964 Validation Accuracy: 0.432600\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     1.1181 Validation Accuracy: 0.437400\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     1.0901 Validation Accuracy: 0.431400\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     1.0728 Validation Accuracy: 0.436800\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     1.0703 Validation Accuracy: 0.433800\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     1.0680 Validation Accuracy: 0.434800\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     1.0581 Validation Accuracy: 0.432600\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     1.0649 Validation Accuracy: 0.436600\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     1.0667 Validation Accuracy: 0.434400\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     1.0314 Validation Accuracy: 0.438600\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     1.0512 Validation Accuracy: 0.434800\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     1.0474 Validation Accuracy: 0.433200\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     1.0517 Validation Accuracy: 0.435800\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     1.0393 Validation Accuracy: 0.440000\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     1.0467 Validation Accuracy: 0.434600\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     1.0327 Validation Accuracy: 0.437400\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     1.0166 Validation Accuracy: 0.437600\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     1.0348 Validation Accuracy: 0.439400\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     1.0048 Validation Accuracy: 0.440600\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     1.0143 Validation Accuracy: 0.439200\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     1.0076 Validation Accuracy: 0.437600\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.9965 Validation Accuracy: 0.437600\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.9927 Validation Accuracy: 0.436600\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.9703 Validation Accuracy: 0.441600\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.9936 Validation Accuracy: 0.436800\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.9781 Validation Accuracy: 0.437400\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.9721 Validation Accuracy: 0.442200\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.9765 Validation Accuracy: 0.443400\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.9733 Validation Accuracy: 0.436400\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.9692 Validation Accuracy: 0.434200\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.9847 Validation Accuracy: 0.443000\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.9684 Validation Accuracy: 0.439400\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.9626 Validation Accuracy: 0.435800\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.9611 Validation Accuracy: 0.441000\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.9558 Validation Accuracy: 0.440000\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.9457 Validation Accuracy: 0.438200\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.9648 Validation Accuracy: 0.433200\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.9428 Validation Accuracy: 0.434200\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.9500 Validation Accuracy: 0.435200\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.9433 Validation Accuracy: 0.441000\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.9450 Validation Accuracy: 0.444800\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.9481 Validation Accuracy: 0.440600\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.9441 Validation Accuracy: 0.439200\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.9369 Validation Accuracy: 0.439000\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.9354 Validation Accuracy: 0.436000\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.9406 Validation Accuracy: 0.433400\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.9368 Validation Accuracy: 0.442400\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.9370 Validation Accuracy: 0.433600\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.9294 Validation Accuracy: 0.440600\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.9197 Validation Accuracy: 0.438000\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.9191 Validation Accuracy: 0.433400\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.9291 Validation Accuracy: 0.437800\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.9228 Validation Accuracy: 0.436800\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.9266 Validation Accuracy: 0.435000\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.9213 Validation Accuracy: 0.433400\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.9213 Validation Accuracy: 0.441800\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.9141 Validation Accuracy: 0.439800\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.9012 Validation Accuracy: 0.440000\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.9084 Validation Accuracy: 0.439600\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.9108 Validation Accuracy: 0.434800\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.8931 Validation Accuracy: 0.439200\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.9051 Validation Accuracy: 0.437400\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:     0.8949 Validation Accuracy: 0.437600\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:     0.8909 Validation Accuracy: 0.436800\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:     0.9006 Validation Accuracy: 0.438200\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:     0.8921 Validation Accuracy: 0.438400\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:     0.9086 Validation Accuracy: 0.439600\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:     0.8932 Validation Accuracy: 0.439400\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:     0.8990 Validation Accuracy: 0.436600\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:     0.9094 Validation Accuracy: 0.434600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109, CIFAR-10 Batch 1:  Loss:     0.9000 Validation Accuracy: 0.440200\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:     0.8794 Validation Accuracy: 0.437000\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:     0.8982 Validation Accuracy: 0.438800\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:     0.8780 Validation Accuracy: 0.441800\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:     0.8780 Validation Accuracy: 0.436600\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:     0.8910 Validation Accuracy: 0.432600\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:     0.8991 Validation Accuracy: 0.431800\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:     0.8849 Validation Accuracy: 0.445400\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:     0.8923 Validation Accuracy: 0.443000\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:     0.8818 Validation Accuracy: 0.437000\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:     0.8768 Validation Accuracy: 0.437000\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:     0.8748 Validation Accuracy: 0.439600\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:     0.8783 Validation Accuracy: 0.436400\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:     0.8835 Validation Accuracy: 0.439600\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:     0.8707 Validation Accuracy: 0.437000\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:     0.8454 Validation Accuracy: 0.441000\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:     0.8486 Validation Accuracy: 0.437000\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:     0.8537 Validation Accuracy: 0.440800\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:     0.8538 Validation Accuracy: 0.437600\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:     0.8423 Validation Accuracy: 0.435600\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss:     0.8233 Validation Accuracy: 0.436800\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss:     0.8475 Validation Accuracy: 0.434200\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss:     0.8330 Validation Accuracy: 0.439200\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss:     0.8444 Validation Accuracy: 0.432200\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss:     0.8502 Validation Accuracy: 0.437000\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss:     0.8490 Validation Accuracy: 0.441800\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss:     0.8267 Validation Accuracy: 0.437800\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss:     0.8256 Validation Accuracy: 0.442600\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss:     0.8260 Validation Accuracy: 0.441400\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss:     0.8213 Validation Accuracy: 0.436400\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss:     0.8082 Validation Accuracy: 0.437400\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss:     0.8069 Validation Accuracy: 0.438400\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss:     0.8098 Validation Accuracy: 0.437200\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss:     0.8021 Validation Accuracy: 0.431200\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss:     0.8029 Validation Accuracy: 0.439000\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss:     0.7909 Validation Accuracy: 0.440000\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss:     0.8104 Validation Accuracy: 0.433800\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss:     0.8043 Validation Accuracy: 0.436400\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss:     0.7950 Validation Accuracy: 0.440200\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss:     0.7933 Validation Accuracy: 0.436800\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss:     0.7886 Validation Accuracy: 0.440800\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss:     0.7997 Validation Accuracy: 0.440200\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss:     0.7952 Validation Accuracy: 0.435000\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss:     0.7991 Validation Accuracy: 0.440400\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss:     0.7927 Validation Accuracy: 0.441200\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss:     0.7792 Validation Accuracy: 0.434800\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss:     0.7933 Validation Accuracy: 0.442200\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss:     0.7900 Validation Accuracy: 0.440000\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss:     0.8015 Validation Accuracy: 0.436200\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss:     0.7908 Validation Accuracy: 0.430800\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss:     0.7893 Validation Accuracy: 0.435800\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss:     0.7881 Validation Accuracy: 0.431800\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss:     0.7922 Validation Accuracy: 0.441600\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss:     0.7917 Validation Accuracy: 0.438600\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss:     0.7724 Validation Accuracy: 0.428600\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss:     0.7794 Validation Accuracy: 0.433400\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss:     0.7855 Validation Accuracy: 0.436800\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss:     0.7861 Validation Accuracy: 0.436600\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss:     0.7953 Validation Accuracy: 0.439400\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss:     0.7789 Validation Accuracy: 0.432800\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss:     0.7762 Validation Accuracy: 0.439800\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss:     0.7715 Validation Accuracy: 0.437400\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss:     0.7849 Validation Accuracy: 0.438400\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss:     0.7773 Validation Accuracy: 0.439400\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss:     0.7742 Validation Accuracy: 0.436800\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss:     0.7747 Validation Accuracy: 0.433000\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss:     0.7736 Validation Accuracy: 0.434200\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss:     0.7619 Validation Accuracy: 0.432400\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss:     0.7709 Validation Accuracy: 0.436000\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss:     0.7683 Validation Accuracy: 0.436600\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss:     0.7754 Validation Accuracy: 0.434200\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss:     0.7696 Validation Accuracy: 0.436600\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss:     0.7692 Validation Accuracy: 0.435000\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss:     0.7772 Validation Accuracy: 0.434400\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss:     0.7644 Validation Accuracy: 0.435200\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss:     0.7665 Validation Accuracy: 0.432400\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss:     0.7611 Validation Accuracy: 0.435200\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss:     0.7622 Validation Accuracy: 0.435000\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss:     0.7701 Validation Accuracy: 0.433800\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss:     0.7659 Validation Accuracy: 0.439800\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss:     0.7627 Validation Accuracy: 0.442600\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss:     0.7592 Validation Accuracy: 0.439200\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss:     0.7644 Validation Accuracy: 0.440600\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss:     0.7626 Validation Accuracy: 0.438800\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss:     0.7760 Validation Accuracy: 0.439400\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss:     0.7590 Validation Accuracy: 0.440000\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss:     0.7628 Validation Accuracy: 0.435600\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss:     0.7662 Validation Accuracy: 0.441400\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss:     0.7624 Validation Accuracy: 0.440800\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss:     0.7642 Validation Accuracy: 0.443000\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss:     0.7605 Validation Accuracy: 0.438000\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss:     0.7559 Validation Accuracy: 0.438400\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.3026 Validation Accuracy: 0.098200\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     2.1965 Validation Accuracy: 0.094200\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     2.2756 Validation Accuracy: 0.151200\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     2.1720 Validation Accuracy: 0.107200\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     2.2767 Validation Accuracy: 0.154400\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.2267 Validation Accuracy: 0.116800\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     2.1643 Validation Accuracy: 0.115000\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     2.1767 Validation Accuracy: 0.111800\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     2.1587 Validation Accuracy: 0.112600\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     2.2750 Validation Accuracy: 0.155600\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     2.1919 Validation Accuracy: 0.105800\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     2.1580 Validation Accuracy: 0.121400\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     2.1366 Validation Accuracy: 0.117400\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     2.1564 Validation Accuracy: 0.126600\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     2.2513 Validation Accuracy: 0.168400\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     2.1723 Validation Accuracy: 0.105600\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     2.1751 Validation Accuracy: 0.123800\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     2.1131 Validation Accuracy: 0.172800\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     2.1425 Validation Accuracy: 0.130000\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     2.2236 Validation Accuracy: 0.169000\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     2.1654 Validation Accuracy: 0.103000\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     2.1649 Validation Accuracy: 0.128800\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     2.0963 Validation Accuracy: 0.172000\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     2.1403 Validation Accuracy: 0.114800\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     2.2096 Validation Accuracy: 0.169800\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     2.1625 Validation Accuracy: 0.176000\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     2.1673 Validation Accuracy: 0.131400\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     2.0820 Validation Accuracy: 0.170400\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     2.1348 Validation Accuracy: 0.134600\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     2.2089 Validation Accuracy: 0.134800\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     2.1602 Validation Accuracy: 0.171800\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     2.1821 Validation Accuracy: 0.125600\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     2.0752 Validation Accuracy: 0.168800\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     2.1376 Validation Accuracy: 0.139400\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     2.1953 Validation Accuracy: 0.128200\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     2.1561 Validation Accuracy: 0.185000\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     2.1695 Validation Accuracy: 0.131000\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     2.0677 Validation Accuracy: 0.173200\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     2.1280 Validation Accuracy: 0.129400\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     2.1966 Validation Accuracy: 0.176600\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     2.1532 Validation Accuracy: 0.181000\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     2.1643 Validation Accuracy: 0.129400\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     2.0582 Validation Accuracy: 0.120400\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     2.1317 Validation Accuracy: 0.177600\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     2.1939 Validation Accuracy: 0.150400\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     2.1549 Validation Accuracy: 0.162000\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     2.1241 Validation Accuracy: 0.210000\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     1.9206 Validation Accuracy: 0.207600\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     2.0467 Validation Accuracy: 0.223000\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     1.8972 Validation Accuracy: 0.296200\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.8110 Validation Accuracy: 0.292000\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     1.8398 Validation Accuracy: 0.293000\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     1.6833 Validation Accuracy: 0.309400\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     1.6305 Validation Accuracy: 0.338400\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     1.5360 Validation Accuracy: 0.348600\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.6614 Validation Accuracy: 0.369200\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     1.7265 Validation Accuracy: 0.399800\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     1.5304 Validation Accuracy: 0.403000\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     1.5566 Validation Accuracy: 0.418400\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     1.4619 Validation Accuracy: 0.425600\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.5006 Validation Accuracy: 0.435200\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     1.5574 Validation Accuracy: 0.441600\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     1.4262 Validation Accuracy: 0.436800\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     1.4501 Validation Accuracy: 0.460000\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     1.3643 Validation Accuracy: 0.453000\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.4089 Validation Accuracy: 0.463600\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     1.5008 Validation Accuracy: 0.464200\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     1.3729 Validation Accuracy: 0.453400\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     1.3987 Validation Accuracy: 0.479800\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     1.2750 Validation Accuracy: 0.465800\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.3908 Validation Accuracy: 0.487200\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     1.4756 Validation Accuracy: 0.479000\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     1.3377 Validation Accuracy: 0.482000\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     1.3334 Validation Accuracy: 0.497800\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     1.1658 Validation Accuracy: 0.501600\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.3378 Validation Accuracy: 0.503400\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     1.3541 Validation Accuracy: 0.496600\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     1.2496 Validation Accuracy: 0.500200\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     1.3148 Validation Accuracy: 0.516400\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     1.1250 Validation Accuracy: 0.516600\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.2396 Validation Accuracy: 0.514800\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     1.3291 Validation Accuracy: 0.513600\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     1.1157 Validation Accuracy: 0.514000\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     1.2660 Validation Accuracy: 0.523800\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     1.0950 Validation Accuracy: 0.530200\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.2159 Validation Accuracy: 0.532400\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     1.2246 Validation Accuracy: 0.530000\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     1.0654 Validation Accuracy: 0.525400\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     1.2418 Validation Accuracy: 0.539400\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     1.0837 Validation Accuracy: 0.536800\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.1773 Validation Accuracy: 0.550000\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     1.0585 Validation Accuracy: 0.540000\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     0.9953 Validation Accuracy: 0.544600\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     1.1286 Validation Accuracy: 0.553600\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     1.0537 Validation Accuracy: 0.554800\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.1424 Validation Accuracy: 0.556600\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     1.0229 Validation Accuracy: 0.555200\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     0.9353 Validation Accuracy: 0.553000\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     1.0778 Validation Accuracy: 0.560600\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     1.0252 Validation Accuracy: 0.559000\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.1055 Validation Accuracy: 0.560400\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:     0.9979 Validation Accuracy: 0.560600\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:     0.9118 Validation Accuracy: 0.562400\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:     1.0583 Validation Accuracy: 0.565000\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:     1.0241 Validation Accuracy: 0.568000\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     1.0678 Validation Accuracy: 0.567600\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:     0.9702 Validation Accuracy: 0.563000\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:     0.9012 Validation Accuracy: 0.562600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, CIFAR-10 Batch 4:  Loss:     1.0510 Validation Accuracy: 0.569000\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:     1.0076 Validation Accuracy: 0.571800\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     1.0660 Validation Accuracy: 0.569400\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:     0.9788 Validation Accuracy: 0.564000\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:     0.8645 Validation Accuracy: 0.567000\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:     1.0152 Validation Accuracy: 0.567800\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:     0.9919 Validation Accuracy: 0.575400\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     1.0432 Validation Accuracy: 0.577000\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:     0.9587 Validation Accuracy: 0.568000\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:     0.8502 Validation Accuracy: 0.574800\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:     1.0141 Validation Accuracy: 0.573200\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:     0.9808 Validation Accuracy: 0.576200\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     0.9964 Validation Accuracy: 0.578800\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:     0.9415 Validation Accuracy: 0.566800\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:     0.8367 Validation Accuracy: 0.574600\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:     0.9814 Validation Accuracy: 0.577400\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:     0.9592 Validation Accuracy: 0.579400\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     0.9849 Validation Accuracy: 0.580400\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:     0.9269 Validation Accuracy: 0.573000\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:     0.8160 Validation Accuracy: 0.576000\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:     0.9778 Validation Accuracy: 0.582000\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:     0.9324 Validation Accuracy: 0.584200\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     0.9840 Validation Accuracy: 0.579400\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:     0.9266 Validation Accuracy: 0.575800\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:     0.8081 Validation Accuracy: 0.579400\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:     0.9549 Validation Accuracy: 0.582600\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:     0.9249 Validation Accuracy: 0.585000\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     0.9694 Validation Accuracy: 0.585600\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:     0.9248 Validation Accuracy: 0.579600\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:     0.7996 Validation Accuracy: 0.579600\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:     0.9338 Validation Accuracy: 0.582200\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:     0.9051 Validation Accuracy: 0.583600\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     0.9622 Validation Accuracy: 0.587000\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:     0.9111 Validation Accuracy: 0.580800\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:     0.7852 Validation Accuracy: 0.586200\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:     0.9167 Validation Accuracy: 0.587000\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:     0.8803 Validation Accuracy: 0.589000\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     0.9443 Validation Accuracy: 0.591200\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:     0.9084 Validation Accuracy: 0.585600\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:     0.7882 Validation Accuracy: 0.587400\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:     0.8933 Validation Accuracy: 0.592200\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:     0.8746 Validation Accuracy: 0.590000\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     0.9531 Validation Accuracy: 0.594400\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:     0.8825 Validation Accuracy: 0.581600\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:     0.7731 Validation Accuracy: 0.587600\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:     0.8971 Validation Accuracy: 0.593000\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:     0.8525 Validation Accuracy: 0.589800\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     0.9200 Validation Accuracy: 0.594600\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:     0.8721 Validation Accuracy: 0.587000\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:     0.7704 Validation Accuracy: 0.589000\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:     0.8727 Validation Accuracy: 0.594600\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:     0.8388 Validation Accuracy: 0.594000\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     0.9150 Validation Accuracy: 0.598400\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:     0.8729 Validation Accuracy: 0.593200\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:     0.7570 Validation Accuracy: 0.591800\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:     0.8350 Validation Accuracy: 0.595600\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:     0.8352 Validation Accuracy: 0.595000\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     0.9247 Validation Accuracy: 0.597400\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:     0.8529 Validation Accuracy: 0.590200\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:     0.7444 Validation Accuracy: 0.590000\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:     0.8398 Validation Accuracy: 0.597800\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:     0.8260 Validation Accuracy: 0.598000\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     0.9101 Validation Accuracy: 0.598600\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:     0.8554 Validation Accuracy: 0.593600\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:     0.7390 Validation Accuracy: 0.593000\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:     0.8230 Validation Accuracy: 0.594200\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:     0.8072 Validation Accuracy: 0.596600\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     0.8850 Validation Accuracy: 0.600600\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:     0.8473 Validation Accuracy: 0.596000\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:     0.7251 Validation Accuracy: 0.593400\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:     0.8100 Validation Accuracy: 0.596400\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:     0.7947 Validation Accuracy: 0.599600\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     0.8658 Validation Accuracy: 0.597000\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:     0.8336 Validation Accuracy: 0.599400\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:     0.7242 Validation Accuracy: 0.596200\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:     0.8198 Validation Accuracy: 0.600600\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:     0.7750 Validation Accuracy: 0.598200\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     0.8897 Validation Accuracy: 0.599000\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:     0.8208 Validation Accuracy: 0.597200\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:     0.7203 Validation Accuracy: 0.599600\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:     0.7994 Validation Accuracy: 0.595400\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:     0.7610 Validation Accuracy: 0.599200\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     0.8721 Validation Accuracy: 0.601600\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:     0.8230 Validation Accuracy: 0.596800\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:     0.7013 Validation Accuracy: 0.599000\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:     0.7998 Validation Accuracy: 0.600600\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:     0.7571 Validation Accuracy: 0.602200\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     0.8621 Validation Accuracy: 0.599600\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:     0.8133 Validation Accuracy: 0.603000\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:     0.7007 Validation Accuracy: 0.603000\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:     0.7777 Validation Accuracy: 0.601200\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:     0.7578 Validation Accuracy: 0.604000\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     0.8577 Validation Accuracy: 0.599200\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:     0.8248 Validation Accuracy: 0.603800\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:     0.6973 Validation Accuracy: 0.602800\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:     0.7565 Validation Accuracy: 0.599200\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:     0.7531 Validation Accuracy: 0.596400\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     0.8664 Validation Accuracy: 0.600800\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:     0.8202 Validation Accuracy: 0.600800\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:     0.6844 Validation Accuracy: 0.604600\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:     0.7496 Validation Accuracy: 0.605400\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:     0.7370 Validation Accuracy: 0.604400\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     0.8738 Validation Accuracy: 0.603600\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:     0.7966 Validation Accuracy: 0.603400\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:     0.6815 Validation Accuracy: 0.604200\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:     0.7599 Validation Accuracy: 0.604800\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss:     0.7364 Validation Accuracy: 0.602800\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     0.8625 Validation Accuracy: 0.602200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44, CIFAR-10 Batch 2:  Loss:     0.7891 Validation Accuracy: 0.604800\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:     0.6692 Validation Accuracy: 0.605800\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:     0.7398 Validation Accuracy: 0.607800\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:     0.7224 Validation Accuracy: 0.602400\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     0.8498 Validation Accuracy: 0.605400\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:     0.7835 Validation Accuracy: 0.605200\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:     0.6695 Validation Accuracy: 0.605200\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:     0.7359 Validation Accuracy: 0.602600\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:     0.7270 Validation Accuracy: 0.603000\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     0.8511 Validation Accuracy: 0.605600\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:     0.8038 Validation Accuracy: 0.611000\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:     0.6548 Validation Accuracy: 0.613400\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:     0.7253 Validation Accuracy: 0.608200\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:     0.7125 Validation Accuracy: 0.603800\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     0.8349 Validation Accuracy: 0.603600\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:     0.7668 Validation Accuracy: 0.604400\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:     0.6624 Validation Accuracy: 0.606800\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:     0.7141 Validation Accuracy: 0.607000\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:     0.7055 Validation Accuracy: 0.605000\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     0.8216 Validation Accuracy: 0.605800\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:     0.7949 Validation Accuracy: 0.608200\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:     0.6609 Validation Accuracy: 0.606800\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:     0.7174 Validation Accuracy: 0.611400\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:     0.6879 Validation Accuracy: 0.607600\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     0.8446 Validation Accuracy: 0.607400\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:     0.7815 Validation Accuracy: 0.607400\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:     0.6549 Validation Accuracy: 0.608000\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:     0.7146 Validation Accuracy: 0.609000\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:     0.6804 Validation Accuracy: 0.604800\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     0.8173 Validation Accuracy: 0.603600\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:     0.7599 Validation Accuracy: 0.608800\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:     0.6627 Validation Accuracy: 0.606800\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:     0.7088 Validation Accuracy: 0.607400\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:     0.6871 Validation Accuracy: 0.609000\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     0.8141 Validation Accuracy: 0.606200\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss:     0.7740 Validation Accuracy: 0.608400\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss:     0.6480 Validation Accuracy: 0.609600\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss:     0.7016 Validation Accuracy: 0.609200\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss:     0.6834 Validation Accuracy: 0.607800\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     0.8229 Validation Accuracy: 0.601200\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss:     0.7685 Validation Accuracy: 0.607000\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss:     0.6546 Validation Accuracy: 0.608600\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss:     0.7006 Validation Accuracy: 0.605800\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss:     0.6865 Validation Accuracy: 0.609600\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     0.8186 Validation Accuracy: 0.609200\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss:     0.7743 Validation Accuracy: 0.606600\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss:     0.6428 Validation Accuracy: 0.613000\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss:     0.6929 Validation Accuracy: 0.607800\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss:     0.6869 Validation Accuracy: 0.610400\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     0.8042 Validation Accuracy: 0.607400\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss:     0.7635 Validation Accuracy: 0.608200\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss:     0.6313 Validation Accuracy: 0.609200\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss:     0.6787 Validation Accuracy: 0.608400\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss:     0.6906 Validation Accuracy: 0.608400\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     0.7841 Validation Accuracy: 0.609000\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss:     0.7545 Validation Accuracy: 0.609200\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss:     0.6253 Validation Accuracy: 0.612000\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss:     0.6770 Validation Accuracy: 0.613800\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss:     0.6720 Validation Accuracy: 0.609000\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     0.8109 Validation Accuracy: 0.609600\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss:     0.7532 Validation Accuracy: 0.610400\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss:     0.6152 Validation Accuracy: 0.607200\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss:     0.6726 Validation Accuracy: 0.610400\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss:     0.6615 Validation Accuracy: 0.607600\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     0.7840 Validation Accuracy: 0.609200\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss:     0.7447 Validation Accuracy: 0.608600\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss:     0.6085 Validation Accuracy: 0.612200\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss:     0.6660 Validation Accuracy: 0.611000\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss:     0.6700 Validation Accuracy: 0.609000\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.7958 Validation Accuracy: 0.608000\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss:     0.7497 Validation Accuracy: 0.611800\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss:     0.6226 Validation Accuracy: 0.610800\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss:     0.6622 Validation Accuracy: 0.611800\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss:     0.6724 Validation Accuracy: 0.608600\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.7730 Validation Accuracy: 0.613000\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss:     0.7635 Validation Accuracy: 0.611000\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss:     0.6058 Validation Accuracy: 0.608600\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss:     0.6551 Validation Accuracy: 0.609200\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss:     0.6597 Validation Accuracy: 0.608800\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.7590 Validation Accuracy: 0.611400\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss:     0.7286 Validation Accuracy: 0.613200\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss:     0.6064 Validation Accuracy: 0.613200\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss:     0.6540 Validation Accuracy: 0.612600\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss:     0.6581 Validation Accuracy: 0.611000\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.7891 Validation Accuracy: 0.612400\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss:     0.7414 Validation Accuracy: 0.613800\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss:     0.5945 Validation Accuracy: 0.613800\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss:     0.6493 Validation Accuracy: 0.611400\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss:     0.6569 Validation Accuracy: 0.605600\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.7807 Validation Accuracy: 0.612000\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss:     0.7264 Validation Accuracy: 0.612400\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss:     0.5912 Validation Accuracy: 0.612800\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss:     0.6469 Validation Accuracy: 0.612400\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss:     0.6419 Validation Accuracy: 0.612800\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.7677 Validation Accuracy: 0.610400\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss:     0.7240 Validation Accuracy: 0.615000\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss:     0.5942 Validation Accuracy: 0.614800\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss:     0.6413 Validation Accuracy: 0.615600\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss:     0.6414 Validation Accuracy: 0.610400\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.7568 Validation Accuracy: 0.612800\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss:     0.7293 Validation Accuracy: 0.614600\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss:     0.5821 Validation Accuracy: 0.612400\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss:     0.6357 Validation Accuracy: 0.614000\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss:     0.6491 Validation Accuracy: 0.615600\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.7417 Validation Accuracy: 0.614200\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss:     0.7284 Validation Accuracy: 0.614800\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss:     0.5926 Validation Accuracy: 0.610800\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss:     0.6235 Validation Accuracy: 0.616000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65, CIFAR-10 Batch 5:  Loss:     0.6412 Validation Accuracy: 0.606400\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.7447 Validation Accuracy: 0.608600\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss:     0.7358 Validation Accuracy: 0.617200\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss:     0.5842 Validation Accuracy: 0.613600\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss:     0.6314 Validation Accuracy: 0.611200\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss:     0.6447 Validation Accuracy: 0.608000\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.7592 Validation Accuracy: 0.613600\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss:     0.7092 Validation Accuracy: 0.615200\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss:     0.5854 Validation Accuracy: 0.618400\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss:     0.6244 Validation Accuracy: 0.612600\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss:     0.6327 Validation Accuracy: 0.614600\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.7381 Validation Accuracy: 0.611400\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss:     0.7199 Validation Accuracy: 0.615000\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss:     0.5851 Validation Accuracy: 0.616800\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss:     0.6300 Validation Accuracy: 0.616200\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss:     0.6256 Validation Accuracy: 0.612200\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.7366 Validation Accuracy: 0.610400\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss:     0.7137 Validation Accuracy: 0.615000\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss:     0.5684 Validation Accuracy: 0.617200\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss:     0.6217 Validation Accuracy: 0.615600\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss:     0.6255 Validation Accuracy: 0.614200\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.7405 Validation Accuracy: 0.614600\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss:     0.7190 Validation Accuracy: 0.619400\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss:     0.5638 Validation Accuracy: 0.614800\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss:     0.6225 Validation Accuracy: 0.615000\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss:     0.6167 Validation Accuracy: 0.613000\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.7385 Validation Accuracy: 0.613000\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss:     0.6979 Validation Accuracy: 0.614400\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss:     0.5767 Validation Accuracy: 0.610000\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss:     0.6121 Validation Accuracy: 0.613000\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss:     0.6078 Validation Accuracy: 0.608200\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.7362 Validation Accuracy: 0.612200\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss:     0.6951 Validation Accuracy: 0.616600\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss:     0.5708 Validation Accuracy: 0.619200\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss:     0.6062 Validation Accuracy: 0.617000\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss:     0.6202 Validation Accuracy: 0.613200\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.7227 Validation Accuracy: 0.611800\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss:     0.6922 Validation Accuracy: 0.616000\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss:     0.5750 Validation Accuracy: 0.615800\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss:     0.6025 Validation Accuracy: 0.617800\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss:     0.6047 Validation Accuracy: 0.611200\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.7206 Validation Accuracy: 0.614200\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss:     0.7020 Validation Accuracy: 0.619800\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss:     0.5440 Validation Accuracy: 0.618400\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss:     0.6034 Validation Accuracy: 0.614400\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss:     0.6083 Validation Accuracy: 0.611200\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.7211 Validation Accuracy: 0.613200\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss:     0.7098 Validation Accuracy: 0.617600\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss:     0.5524 Validation Accuracy: 0.617000\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss:     0.6005 Validation Accuracy: 0.613600\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss:     0.6140 Validation Accuracy: 0.611400\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.7474 Validation Accuracy: 0.613200\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss:     0.6851 Validation Accuracy: 0.618200\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss:     0.5419 Validation Accuracy: 0.617400\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss:     0.5857 Validation Accuracy: 0.612000\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss:     0.6065 Validation Accuracy: 0.611400\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.6932 Validation Accuracy: 0.613600\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss:     0.6775 Validation Accuracy: 0.616600\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss:     0.5549 Validation Accuracy: 0.616000\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss:     0.5947 Validation Accuracy: 0.616200\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss:     0.6013 Validation Accuracy: 0.608400\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.7164 Validation Accuracy: 0.614200\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss:     0.6834 Validation Accuracy: 0.615600\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss:     0.5317 Validation Accuracy: 0.616200\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss:     0.5830 Validation Accuracy: 0.614800\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss:     0.6059 Validation Accuracy: 0.613800\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.7053 Validation Accuracy: 0.616600\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss:     0.6871 Validation Accuracy: 0.616200\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss:     0.5362 Validation Accuracy: 0.613800\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss:     0.5905 Validation Accuracy: 0.614400\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss:     0.5961 Validation Accuracy: 0.609200\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.7125 Validation Accuracy: 0.613200\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss:     0.6606 Validation Accuracy: 0.619400\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss:     0.5306 Validation Accuracy: 0.615800\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss:     0.5918 Validation Accuracy: 0.616600\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss:     0.5956 Validation Accuracy: 0.613000\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.7201 Validation Accuracy: 0.614200\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss:     0.6706 Validation Accuracy: 0.616600\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss:     0.5377 Validation Accuracy: 0.619400\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss:     0.5923 Validation Accuracy: 0.619600\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss:     0.5902 Validation Accuracy: 0.611600\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.6909 Validation Accuracy: 0.613000\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss:     0.6784 Validation Accuracy: 0.615400\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss:     0.5491 Validation Accuracy: 0.617400\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss:     0.5835 Validation Accuracy: 0.618800\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss:     0.5988 Validation Accuracy: 0.610400\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.6984 Validation Accuracy: 0.614200\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss:     0.6854 Validation Accuracy: 0.617800\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss:     0.5224 Validation Accuracy: 0.618400\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss:     0.5795 Validation Accuracy: 0.615200\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss:     0.5989 Validation Accuracy: 0.608400\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.7091 Validation Accuracy: 0.616800\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss:     0.6618 Validation Accuracy: 0.619000\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss:     0.5326 Validation Accuracy: 0.616400\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss:     0.5824 Validation Accuracy: 0.615400\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss:     0.5916 Validation Accuracy: 0.613000\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.7055 Validation Accuracy: 0.610600\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss:     0.6890 Validation Accuracy: 0.615400\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss:     0.5408 Validation Accuracy: 0.617400\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss:     0.5756 Validation Accuracy: 0.616000\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss:     0.5765 Validation Accuracy: 0.610400\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.7116 Validation Accuracy: 0.615800\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss:     0.6597 Validation Accuracy: 0.615200\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss:     0.5295 Validation Accuracy: 0.615400\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss:     0.5864 Validation Accuracy: 0.615600\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss:     0.5842 Validation Accuracy: 0.607200\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.6810 Validation Accuracy: 0.614000\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss:     0.6613 Validation Accuracy: 0.618200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87, CIFAR-10 Batch 3:  Loss:     0.5251 Validation Accuracy: 0.620800\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss:     0.5694 Validation Accuracy: 0.614600\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss:     0.5761 Validation Accuracy: 0.608000\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.6803 Validation Accuracy: 0.615400\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss:     0.6634 Validation Accuracy: 0.616200\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss:     0.5277 Validation Accuracy: 0.619800\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss:     0.5848 Validation Accuracy: 0.617000\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss:     0.5693 Validation Accuracy: 0.610800\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.6929 Validation Accuracy: 0.616600\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss:     0.6557 Validation Accuracy: 0.619600\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss:     0.5182 Validation Accuracy: 0.616600\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss:     0.5614 Validation Accuracy: 0.618000\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss:     0.5724 Validation Accuracy: 0.613200\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.6929 Validation Accuracy: 0.614800\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss:     0.6534 Validation Accuracy: 0.616400\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss:     0.5208 Validation Accuracy: 0.616400\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss:     0.5541 Validation Accuracy: 0.618800\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss:     0.5677 Validation Accuracy: 0.611000\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.6858 Validation Accuracy: 0.621000\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss:     0.6751 Validation Accuracy: 0.619600\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss:     0.5321 Validation Accuracy: 0.617200\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss:     0.5629 Validation Accuracy: 0.615600\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss:     0.5809 Validation Accuracy: 0.614600\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.6935 Validation Accuracy: 0.615200\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss:     0.6627 Validation Accuracy: 0.621000\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss:     0.5181 Validation Accuracy: 0.620000\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss:     0.5557 Validation Accuracy: 0.619000\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss:     0.5859 Validation Accuracy: 0.613400\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.6758 Validation Accuracy: 0.617400\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss:     0.6698 Validation Accuracy: 0.620000\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss:     0.5077 Validation Accuracy: 0.617200\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss:     0.5629 Validation Accuracy: 0.615800\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss:     0.5887 Validation Accuracy: 0.614200\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.6866 Validation Accuracy: 0.619000\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss:     0.6553 Validation Accuracy: 0.619000\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss:     0.5023 Validation Accuracy: 0.621600\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss:     0.5608 Validation Accuracy: 0.619000\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss:     0.5714 Validation Accuracy: 0.612600\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.6797 Validation Accuracy: 0.617600\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss:     0.6471 Validation Accuracy: 0.621800\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss:     0.5130 Validation Accuracy: 0.618000\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss:     0.5585 Validation Accuracy: 0.616000\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss:     0.5740 Validation Accuracy: 0.614200\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.6695 Validation Accuracy: 0.617800\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss:     0.6526 Validation Accuracy: 0.620400\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss:     0.5011 Validation Accuracy: 0.620200\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss:     0.5596 Validation Accuracy: 0.621000\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss:     0.5432 Validation Accuracy: 0.611000\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.6699 Validation Accuracy: 0.618200\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss:     0.6584 Validation Accuracy: 0.622200\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss:     0.5106 Validation Accuracy: 0.622200\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss:     0.5568 Validation Accuracy: 0.618200\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss:     0.5619 Validation Accuracy: 0.612400\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.6737 Validation Accuracy: 0.617600\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss:     0.6578 Validation Accuracy: 0.621600\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss:     0.5041 Validation Accuracy: 0.620000\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss:     0.5513 Validation Accuracy: 0.624000\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss:     0.5714 Validation Accuracy: 0.611400\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.6668 Validation Accuracy: 0.618200\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss:     0.6311 Validation Accuracy: 0.620200\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss:     0.5081 Validation Accuracy: 0.621600\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss:     0.5515 Validation Accuracy: 0.620600\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss:     0.5591 Validation Accuracy: 0.609800\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.6763 Validation Accuracy: 0.622000\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss:     0.6411 Validation Accuracy: 0.621600\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss:     0.5009 Validation Accuracy: 0.619800\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss:     0.5490 Validation Accuracy: 0.618800\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss:     0.5582 Validation Accuracy: 0.609200\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:     0.6628 Validation Accuracy: 0.620200\n",
      "Epoch 101, CIFAR-10 Batch 2:  Loss:     0.6357 Validation Accuracy: 0.615600\n",
      "Epoch 101, CIFAR-10 Batch 3:  Loss:     0.4973 Validation Accuracy: 0.620200\n",
      "Epoch 101, CIFAR-10 Batch 4:  Loss:     0.5533 Validation Accuracy: 0.620200\n",
      "Epoch 101, CIFAR-10 Batch 5:  Loss:     0.5434 Validation Accuracy: 0.614000\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:     0.6796 Validation Accuracy: 0.619800\n",
      "Epoch 102, CIFAR-10 Batch 2:  Loss:     0.6490 Validation Accuracy: 0.617400\n",
      "Epoch 102, CIFAR-10 Batch 3:  Loss:     0.5011 Validation Accuracy: 0.623600\n",
      "Epoch 102, CIFAR-10 Batch 4:  Loss:     0.5519 Validation Accuracy: 0.621400\n",
      "Epoch 102, CIFAR-10 Batch 5:  Loss:     0.5550 Validation Accuracy: 0.615600\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:     0.6579 Validation Accuracy: 0.619400\n",
      "Epoch 103, CIFAR-10 Batch 2:  Loss:     0.6552 Validation Accuracy: 0.622800\n",
      "Epoch 103, CIFAR-10 Batch 3:  Loss:     0.4900 Validation Accuracy: 0.620800\n",
      "Epoch 103, CIFAR-10 Batch 4:  Loss:     0.5462 Validation Accuracy: 0.619200\n",
      "Epoch 103, CIFAR-10 Batch 5:  Loss:     0.5315 Validation Accuracy: 0.615400\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:     0.6557 Validation Accuracy: 0.616600\n",
      "Epoch 104, CIFAR-10 Batch 2:  Loss:     0.6437 Validation Accuracy: 0.619000\n",
      "Epoch 104, CIFAR-10 Batch 3:  Loss:     0.5025 Validation Accuracy: 0.622000\n",
      "Epoch 104, CIFAR-10 Batch 4:  Loss:     0.5284 Validation Accuracy: 0.620200\n",
      "Epoch 104, CIFAR-10 Batch 5:  Loss:     0.5500 Validation Accuracy: 0.617000\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:     0.6612 Validation Accuracy: 0.618800\n",
      "Epoch 105, CIFAR-10 Batch 2:  Loss:     0.6479 Validation Accuracy: 0.619400\n",
      "Epoch 105, CIFAR-10 Batch 3:  Loss:     0.4902 Validation Accuracy: 0.624000\n",
      "Epoch 105, CIFAR-10 Batch 4:  Loss:     0.5360 Validation Accuracy: 0.622200\n",
      "Epoch 105, CIFAR-10 Batch 5:  Loss:     0.5472 Validation Accuracy: 0.615800\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:     0.6696 Validation Accuracy: 0.618200\n",
      "Epoch 106, CIFAR-10 Batch 2:  Loss:     0.6375 Validation Accuracy: 0.623000\n",
      "Epoch 106, CIFAR-10 Batch 3:  Loss:     0.4869 Validation Accuracy: 0.622000\n",
      "Epoch 106, CIFAR-10 Batch 4:  Loss:     0.5310 Validation Accuracy: 0.622200\n",
      "Epoch 106, CIFAR-10 Batch 5:  Loss:     0.5483 Validation Accuracy: 0.616200\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:     0.6749 Validation Accuracy: 0.619800\n",
      "Epoch 107, CIFAR-10 Batch 2:  Loss:     0.6483 Validation Accuracy: 0.622000\n",
      "Epoch 107, CIFAR-10 Batch 3:  Loss:     0.4807 Validation Accuracy: 0.622400\n",
      "Epoch 107, CIFAR-10 Batch 4:  Loss:     0.5381 Validation Accuracy: 0.620200\n",
      "Epoch 107, CIFAR-10 Batch 5:  Loss:     0.5453 Validation Accuracy: 0.613400\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:     0.6894 Validation Accuracy: 0.621000\n",
      "Epoch 108, CIFAR-10 Batch 2:  Loss:     0.6299 Validation Accuracy: 0.621000\n",
      "Epoch 108, CIFAR-10 Batch 3:  Loss:     0.4786 Validation Accuracy: 0.620000\n",
      "Epoch 108, CIFAR-10 Batch 4:  Loss:     0.5221 Validation Accuracy: 0.622800\n",
      "Epoch 108, CIFAR-10 Batch 5:  Loss:     0.5408 Validation Accuracy: 0.613600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109, CIFAR-10 Batch 1:  Loss:     0.6556 Validation Accuracy: 0.616400\n",
      "Epoch 109, CIFAR-10 Batch 2:  Loss:     0.6308 Validation Accuracy: 0.620600\n",
      "Epoch 109, CIFAR-10 Batch 3:  Loss:     0.4695 Validation Accuracy: 0.623400\n",
      "Epoch 109, CIFAR-10 Batch 4:  Loss:     0.5421 Validation Accuracy: 0.621600\n",
      "Epoch 109, CIFAR-10 Batch 5:  Loss:     0.5462 Validation Accuracy: 0.613200\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:     0.6505 Validation Accuracy: 0.618000\n",
      "Epoch 110, CIFAR-10 Batch 2:  Loss:     0.6227 Validation Accuracy: 0.624000\n",
      "Epoch 110, CIFAR-10 Batch 3:  Loss:     0.4716 Validation Accuracy: 0.623200\n",
      "Epoch 110, CIFAR-10 Batch 4:  Loss:     0.5391 Validation Accuracy: 0.619000\n",
      "Epoch 110, CIFAR-10 Batch 5:  Loss:     0.5526 Validation Accuracy: 0.613600\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:     0.6613 Validation Accuracy: 0.617800\n",
      "Epoch 111, CIFAR-10 Batch 2:  Loss:     0.6153 Validation Accuracy: 0.622400\n",
      "Epoch 111, CIFAR-10 Batch 3:  Loss:     0.4996 Validation Accuracy: 0.620800\n",
      "Epoch 111, CIFAR-10 Batch 4:  Loss:     0.5277 Validation Accuracy: 0.624200\n",
      "Epoch 111, CIFAR-10 Batch 5:  Loss:     0.5407 Validation Accuracy: 0.614600\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:     0.6641 Validation Accuracy: 0.618200\n",
      "Epoch 112, CIFAR-10 Batch 2:  Loss:     0.6080 Validation Accuracy: 0.622600\n",
      "Epoch 112, CIFAR-10 Batch 3:  Loss:     0.4910 Validation Accuracy: 0.621400\n",
      "Epoch 112, CIFAR-10 Batch 4:  Loss:     0.5316 Validation Accuracy: 0.623000\n",
      "Epoch 112, CIFAR-10 Batch 5:  Loss:     0.5401 Validation Accuracy: 0.613200\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:     0.6716 Validation Accuracy: 0.617800\n",
      "Epoch 113, CIFAR-10 Batch 2:  Loss:     0.6319 Validation Accuracy: 0.625600\n",
      "Epoch 113, CIFAR-10 Batch 3:  Loss:     0.4796 Validation Accuracy: 0.623000\n",
      "Epoch 113, CIFAR-10 Batch 4:  Loss:     0.5370 Validation Accuracy: 0.620800\n",
      "Epoch 113, CIFAR-10 Batch 5:  Loss:     0.5400 Validation Accuracy: 0.616800\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:     0.6595 Validation Accuracy: 0.621400\n",
      "Epoch 114, CIFAR-10 Batch 2:  Loss:     0.6241 Validation Accuracy: 0.622400\n",
      "Epoch 114, CIFAR-10 Batch 3:  Loss:     0.4774 Validation Accuracy: 0.626000\n",
      "Epoch 114, CIFAR-10 Batch 4:  Loss:     0.5418 Validation Accuracy: 0.625000\n",
      "Epoch 114, CIFAR-10 Batch 5:  Loss:     0.5314 Validation Accuracy: 0.617000\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:     0.6458 Validation Accuracy: 0.618400\n",
      "Epoch 115, CIFAR-10 Batch 2:  Loss:     0.6226 Validation Accuracy: 0.623800\n",
      "Epoch 115, CIFAR-10 Batch 3:  Loss:     0.4733 Validation Accuracy: 0.622600\n",
      "Epoch 115, CIFAR-10 Batch 4:  Loss:     0.5177 Validation Accuracy: 0.624200\n",
      "Epoch 115, CIFAR-10 Batch 5:  Loss:     0.5335 Validation Accuracy: 0.615200\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:     0.6622 Validation Accuracy: 0.621400\n",
      "Epoch 116, CIFAR-10 Batch 2:  Loss:     0.6305 Validation Accuracy: 0.624000\n",
      "Epoch 116, CIFAR-10 Batch 3:  Loss:     0.4667 Validation Accuracy: 0.620000\n",
      "Epoch 116, CIFAR-10 Batch 4:  Loss:     0.5326 Validation Accuracy: 0.622600\n",
      "Epoch 116, CIFAR-10 Batch 5:  Loss:     0.5459 Validation Accuracy: 0.612600\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:     0.6551 Validation Accuracy: 0.615400\n",
      "Epoch 117, CIFAR-10 Batch 2:  Loss:     0.6359 Validation Accuracy: 0.621800\n",
      "Epoch 117, CIFAR-10 Batch 3:  Loss:     0.4657 Validation Accuracy: 0.620400\n",
      "Epoch 117, CIFAR-10 Batch 4:  Loss:     0.5249 Validation Accuracy: 0.619400\n",
      "Epoch 117, CIFAR-10 Batch 5:  Loss:     0.5370 Validation Accuracy: 0.614400\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:     0.6557 Validation Accuracy: 0.621800\n",
      "Epoch 118, CIFAR-10 Batch 2:  Loss:     0.6172 Validation Accuracy: 0.625600\n",
      "Epoch 118, CIFAR-10 Batch 3:  Loss:     0.4681 Validation Accuracy: 0.617600\n",
      "Epoch 118, CIFAR-10 Batch 4:  Loss:     0.5259 Validation Accuracy: 0.620800\n",
      "Epoch 118, CIFAR-10 Batch 5:  Loss:     0.5410 Validation Accuracy: 0.613400\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:     0.6387 Validation Accuracy: 0.624200\n",
      "Epoch 119, CIFAR-10 Batch 2:  Loss:     0.6150 Validation Accuracy: 0.623200\n",
      "Epoch 119, CIFAR-10 Batch 3:  Loss:     0.4619 Validation Accuracy: 0.621800\n",
      "Epoch 119, CIFAR-10 Batch 4:  Loss:     0.5245 Validation Accuracy: 0.620000\n",
      "Epoch 119, CIFAR-10 Batch 5:  Loss:     0.5361 Validation Accuracy: 0.616400\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:     0.6481 Validation Accuracy: 0.621000\n",
      "Epoch 120, CIFAR-10 Batch 2:  Loss:     0.6023 Validation Accuracy: 0.621000\n",
      "Epoch 120, CIFAR-10 Batch 3:  Loss:     0.4696 Validation Accuracy: 0.626200\n",
      "Epoch 120, CIFAR-10 Batch 4:  Loss:     0.5249 Validation Accuracy: 0.624400\n",
      "Epoch 120, CIFAR-10 Batch 5:  Loss:     0.5290 Validation Accuracy: 0.620400\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:     0.6503 Validation Accuracy: 0.619800\n",
      "Epoch 121, CIFAR-10 Batch 2:  Loss:     0.5969 Validation Accuracy: 0.624200\n",
      "Epoch 121, CIFAR-10 Batch 3:  Loss:     0.4727 Validation Accuracy: 0.617000\n",
      "Epoch 121, CIFAR-10 Batch 4:  Loss:     0.5317 Validation Accuracy: 0.621800\n",
      "Epoch 121, CIFAR-10 Batch 5:  Loss:     0.5371 Validation Accuracy: 0.615600\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:     0.6436 Validation Accuracy: 0.622200\n",
      "Epoch 122, CIFAR-10 Batch 2:  Loss:     0.5956 Validation Accuracy: 0.622400\n",
      "Epoch 122, CIFAR-10 Batch 3:  Loss:     0.4658 Validation Accuracy: 0.623600\n",
      "Epoch 122, CIFAR-10 Batch 4:  Loss:     0.5202 Validation Accuracy: 0.621800\n",
      "Epoch 122, CIFAR-10 Batch 5:  Loss:     0.5372 Validation Accuracy: 0.619200\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:     0.6422 Validation Accuracy: 0.621200\n",
      "Epoch 123, CIFAR-10 Batch 2:  Loss:     0.6067 Validation Accuracy: 0.620600\n",
      "Epoch 123, CIFAR-10 Batch 3:  Loss:     0.4521 Validation Accuracy: 0.628200\n",
      "Epoch 123, CIFAR-10 Batch 4:  Loss:     0.5234 Validation Accuracy: 0.621800\n",
      "Epoch 123, CIFAR-10 Batch 5:  Loss:     0.5321 Validation Accuracy: 0.620600\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:     0.6405 Validation Accuracy: 0.623600\n",
      "Epoch 124, CIFAR-10 Batch 2:  Loss:     0.6210 Validation Accuracy: 0.622600\n",
      "Epoch 124, CIFAR-10 Batch 3:  Loss:     0.4587 Validation Accuracy: 0.620000\n",
      "Epoch 124, CIFAR-10 Batch 4:  Loss:     0.5178 Validation Accuracy: 0.620800\n",
      "Epoch 124, CIFAR-10 Batch 5:  Loss:     0.5368 Validation Accuracy: 0.615600\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:     0.6503 Validation Accuracy: 0.622000\n",
      "Epoch 125, CIFAR-10 Batch 2:  Loss:     0.5946 Validation Accuracy: 0.622600\n",
      "Epoch 125, CIFAR-10 Batch 3:  Loss:     0.4577 Validation Accuracy: 0.623000\n",
      "Epoch 125, CIFAR-10 Batch 4:  Loss:     0.5064 Validation Accuracy: 0.623800\n",
      "Epoch 125, CIFAR-10 Batch 5:  Loss:     0.5326 Validation Accuracy: 0.616200\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:     0.6396 Validation Accuracy: 0.619600\n",
      "Epoch 126, CIFAR-10 Batch 2:  Loss:     0.5881 Validation Accuracy: 0.626400\n",
      "Epoch 126, CIFAR-10 Batch 3:  Loss:     0.4544 Validation Accuracy: 0.624400\n",
      "Epoch 126, CIFAR-10 Batch 4:  Loss:     0.5033 Validation Accuracy: 0.622600\n",
      "Epoch 126, CIFAR-10 Batch 5:  Loss:     0.5298 Validation Accuracy: 0.613800\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:     0.6419 Validation Accuracy: 0.620400\n",
      "Epoch 127, CIFAR-10 Batch 2:  Loss:     0.5917 Validation Accuracy: 0.624200\n",
      "Epoch 127, CIFAR-10 Batch 3:  Loss:     0.4593 Validation Accuracy: 0.623200\n",
      "Epoch 127, CIFAR-10 Batch 4:  Loss:     0.5148 Validation Accuracy: 0.625200\n",
      "Epoch 127, CIFAR-10 Batch 5:  Loss:     0.5348 Validation Accuracy: 0.617000\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:     0.6341 Validation Accuracy: 0.619000\n",
      "Epoch 128, CIFAR-10 Batch 2:  Loss:     0.5859 Validation Accuracy: 0.622600\n",
      "Epoch 128, CIFAR-10 Batch 3:  Loss:     0.4525 Validation Accuracy: 0.626400\n",
      "Epoch 128, CIFAR-10 Batch 4:  Loss:     0.5120 Validation Accuracy: 0.625000\n",
      "Epoch 128, CIFAR-10 Batch 5:  Loss:     0.5277 Validation Accuracy: 0.614400\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss:     0.6240 Validation Accuracy: 0.620200\n",
      "Epoch 129, CIFAR-10 Batch 2:  Loss:     0.5916 Validation Accuracy: 0.625400\n",
      "Epoch 129, CIFAR-10 Batch 3:  Loss:     0.4453 Validation Accuracy: 0.624600\n",
      "Epoch 129, CIFAR-10 Batch 4:  Loss:     0.5125 Validation Accuracy: 0.622600\n",
      "Epoch 129, CIFAR-10 Batch 5:  Loss:     0.5202 Validation Accuracy: 0.620600\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss:     0.6484 Validation Accuracy: 0.621400\n",
      "Epoch 130, CIFAR-10 Batch 2:  Loss:     0.5885 Validation Accuracy: 0.621800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130, CIFAR-10 Batch 3:  Loss:     0.4668 Validation Accuracy: 0.624800\n",
      "Epoch 130, CIFAR-10 Batch 4:  Loss:     0.5210 Validation Accuracy: 0.618200\n",
      "Epoch 130, CIFAR-10 Batch 5:  Loss:     0.5225 Validation Accuracy: 0.616800\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss:     0.6362 Validation Accuracy: 0.624000\n",
      "Epoch 131, CIFAR-10 Batch 2:  Loss:     0.6030 Validation Accuracy: 0.625800\n",
      "Epoch 131, CIFAR-10 Batch 3:  Loss:     0.4397 Validation Accuracy: 0.628000\n",
      "Epoch 131, CIFAR-10 Batch 4:  Loss:     0.4909 Validation Accuracy: 0.622400\n",
      "Epoch 131, CIFAR-10 Batch 5:  Loss:     0.5123 Validation Accuracy: 0.619800\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss:     0.6275 Validation Accuracy: 0.620000\n",
      "Epoch 132, CIFAR-10 Batch 2:  Loss:     0.5982 Validation Accuracy: 0.622800\n",
      "Epoch 132, CIFAR-10 Batch 3:  Loss:     0.4498 Validation Accuracy: 0.625200\n",
      "Epoch 132, CIFAR-10 Batch 4:  Loss:     0.4967 Validation Accuracy: 0.622000\n",
      "Epoch 132, CIFAR-10 Batch 5:  Loss:     0.4952 Validation Accuracy: 0.617000\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss:     0.6446 Validation Accuracy: 0.622200\n",
      "Epoch 133, CIFAR-10 Batch 2:  Loss:     0.5971 Validation Accuracy: 0.627800\n",
      "Epoch 133, CIFAR-10 Batch 3:  Loss:     0.4524 Validation Accuracy: 0.622200\n",
      "Epoch 133, CIFAR-10 Batch 4:  Loss:     0.5059 Validation Accuracy: 0.622200\n",
      "Epoch 133, CIFAR-10 Batch 5:  Loss:     0.4981 Validation Accuracy: 0.616200\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss:     0.6172 Validation Accuracy: 0.618200\n",
      "Epoch 134, CIFAR-10 Batch 2:  Loss:     0.5908 Validation Accuracy: 0.629200\n",
      "Epoch 134, CIFAR-10 Batch 3:  Loss:     0.4461 Validation Accuracy: 0.625800\n",
      "Epoch 134, CIFAR-10 Batch 4:  Loss:     0.5195 Validation Accuracy: 0.623200\n",
      "Epoch 134, CIFAR-10 Batch 5:  Loss:     0.5245 Validation Accuracy: 0.616000\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss:     0.6384 Validation Accuracy: 0.624800\n",
      "Epoch 135, CIFAR-10 Batch 2:  Loss:     0.5798 Validation Accuracy: 0.623600\n",
      "Epoch 135, CIFAR-10 Batch 3:  Loss:     0.4475 Validation Accuracy: 0.625000\n",
      "Epoch 135, CIFAR-10 Batch 4:  Loss:     0.5181 Validation Accuracy: 0.624800\n",
      "Epoch 135, CIFAR-10 Batch 5:  Loss:     0.5097 Validation Accuracy: 0.620400\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss:     0.6403 Validation Accuracy: 0.619800\n",
      "Epoch 136, CIFAR-10 Batch 2:  Loss:     0.5801 Validation Accuracy: 0.626800\n",
      "Epoch 136, CIFAR-10 Batch 3:  Loss:     0.4457 Validation Accuracy: 0.623200\n",
      "Epoch 136, CIFAR-10 Batch 4:  Loss:     0.5088 Validation Accuracy: 0.623400\n",
      "Epoch 136, CIFAR-10 Batch 5:  Loss:     0.4976 Validation Accuracy: 0.621400\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss:     0.6230 Validation Accuracy: 0.621200\n",
      "Epoch 137, CIFAR-10 Batch 2:  Loss:     0.5794 Validation Accuracy: 0.626800\n",
      "Epoch 137, CIFAR-10 Batch 3:  Loss:     0.4490 Validation Accuracy: 0.624200\n",
      "Epoch 137, CIFAR-10 Batch 4:  Loss:     0.5046 Validation Accuracy: 0.620400\n",
      "Epoch 137, CIFAR-10 Batch 5:  Loss:     0.5044 Validation Accuracy: 0.622400\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss:     0.6155 Validation Accuracy: 0.621000\n",
      "Epoch 138, CIFAR-10 Batch 2:  Loss:     0.5866 Validation Accuracy: 0.625400\n",
      "Epoch 138, CIFAR-10 Batch 3:  Loss:     0.4515 Validation Accuracy: 0.622600\n",
      "Epoch 138, CIFAR-10 Batch 4:  Loss:     0.4936 Validation Accuracy: 0.625800\n",
      "Epoch 138, CIFAR-10 Batch 5:  Loss:     0.5180 Validation Accuracy: 0.616600\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss:     0.6038 Validation Accuracy: 0.621800\n",
      "Epoch 139, CIFAR-10 Batch 2:  Loss:     0.5802 Validation Accuracy: 0.627400\n",
      "Epoch 139, CIFAR-10 Batch 3:  Loss:     0.4589 Validation Accuracy: 0.619200\n",
      "Epoch 139, CIFAR-10 Batch 4:  Loss:     0.4965 Validation Accuracy: 0.622800\n",
      "Epoch 139, CIFAR-10 Batch 5:  Loss:     0.5107 Validation Accuracy: 0.621400\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss:     0.6236 Validation Accuracy: 0.618400\n",
      "Epoch 140, CIFAR-10 Batch 2:  Loss:     0.5877 Validation Accuracy: 0.624000\n",
      "Epoch 140, CIFAR-10 Batch 3:  Loss:     0.4501 Validation Accuracy: 0.623600\n",
      "Epoch 140, CIFAR-10 Batch 4:  Loss:     0.5157 Validation Accuracy: 0.623200\n",
      "Epoch 140, CIFAR-10 Batch 5:  Loss:     0.5067 Validation Accuracy: 0.618800\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss:     0.6156 Validation Accuracy: 0.620200\n",
      "Epoch 141, CIFAR-10 Batch 2:  Loss:     0.5817 Validation Accuracy: 0.625200\n",
      "Epoch 141, CIFAR-10 Batch 3:  Loss:     0.4449 Validation Accuracy: 0.624200\n",
      "Epoch 141, CIFAR-10 Batch 4:  Loss:     0.5185 Validation Accuracy: 0.620000\n",
      "Epoch 141, CIFAR-10 Batch 5:  Loss:     0.4943 Validation Accuracy: 0.619000\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss:     0.6020 Validation Accuracy: 0.620400\n",
      "Epoch 142, CIFAR-10 Batch 2:  Loss:     0.6010 Validation Accuracy: 0.630600\n",
      "Epoch 142, CIFAR-10 Batch 3:  Loss:     0.4397 Validation Accuracy: 0.626000\n",
      "Epoch 142, CIFAR-10 Batch 4:  Loss:     0.5060 Validation Accuracy: 0.621600\n",
      "Epoch 142, CIFAR-10 Batch 5:  Loss:     0.5032 Validation Accuracy: 0.618000\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss:     0.6263 Validation Accuracy: 0.624400\n",
      "Epoch 143, CIFAR-10 Batch 2:  Loss:     0.5897 Validation Accuracy: 0.623800\n",
      "Epoch 143, CIFAR-10 Batch 3:  Loss:     0.4419 Validation Accuracy: 0.628000\n",
      "Epoch 143, CIFAR-10 Batch 4:  Loss:     0.5075 Validation Accuracy: 0.619200\n",
      "Epoch 143, CIFAR-10 Batch 5:  Loss:     0.4887 Validation Accuracy: 0.618200\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss:     0.6055 Validation Accuracy: 0.622800\n",
      "Epoch 144, CIFAR-10 Batch 2:  Loss:     0.5905 Validation Accuracy: 0.627800\n",
      "Epoch 144, CIFAR-10 Batch 3:  Loss:     0.4339 Validation Accuracy: 0.620000\n",
      "Epoch 144, CIFAR-10 Batch 4:  Loss:     0.4995 Validation Accuracy: 0.620000\n",
      "Epoch 144, CIFAR-10 Batch 5:  Loss:     0.5031 Validation Accuracy: 0.617400\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss:     0.6070 Validation Accuracy: 0.622600\n",
      "Epoch 145, CIFAR-10 Batch 2:  Loss:     0.5885 Validation Accuracy: 0.625000\n",
      "Epoch 145, CIFAR-10 Batch 3:  Loss:     0.4420 Validation Accuracy: 0.623000\n",
      "Epoch 145, CIFAR-10 Batch 4:  Loss:     0.4931 Validation Accuracy: 0.621200\n",
      "Epoch 145, CIFAR-10 Batch 5:  Loss:     0.5057 Validation Accuracy: 0.620000\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss:     0.6114 Validation Accuracy: 0.621800\n",
      "Epoch 146, CIFAR-10 Batch 2:  Loss:     0.5808 Validation Accuracy: 0.624400\n",
      "Epoch 146, CIFAR-10 Batch 3:  Loss:     0.4415 Validation Accuracy: 0.623200\n",
      "Epoch 146, CIFAR-10 Batch 4:  Loss:     0.4911 Validation Accuracy: 0.623400\n",
      "Epoch 146, CIFAR-10 Batch 5:  Loss:     0.5034 Validation Accuracy: 0.617600\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss:     0.6241 Validation Accuracy: 0.623800\n",
      "Epoch 147, CIFAR-10 Batch 2:  Loss:     0.5831 Validation Accuracy: 0.626200\n",
      "Epoch 147, CIFAR-10 Batch 3:  Loss:     0.4377 Validation Accuracy: 0.624200\n",
      "Epoch 147, CIFAR-10 Batch 4:  Loss:     0.5140 Validation Accuracy: 0.621600\n",
      "Epoch 147, CIFAR-10 Batch 5:  Loss:     0.5064 Validation Accuracy: 0.619400\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss:     0.6091 Validation Accuracy: 0.624200\n",
      "Epoch 148, CIFAR-10 Batch 2:  Loss:     0.5893 Validation Accuracy: 0.622400\n",
      "Epoch 148, CIFAR-10 Batch 3:  Loss:     0.4329 Validation Accuracy: 0.624200\n",
      "Epoch 148, CIFAR-10 Batch 4:  Loss:     0.5004 Validation Accuracy: 0.621200\n",
      "Epoch 148, CIFAR-10 Batch 5:  Loss:     0.4967 Validation Accuracy: 0.622200\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss:     0.6086 Validation Accuracy: 0.621400\n",
      "Epoch 149, CIFAR-10 Batch 2:  Loss:     0.5810 Validation Accuracy: 0.625800\n",
      "Epoch 149, CIFAR-10 Batch 3:  Loss:     0.4290 Validation Accuracy: 0.623800\n",
      "Epoch 149, CIFAR-10 Batch 4:  Loss:     0.4982 Validation Accuracy: 0.624200\n",
      "Epoch 149, CIFAR-10 Batch 5:  Loss:     0.4908 Validation Accuracy: 0.621600\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss:     0.6158 Validation Accuracy: 0.620600\n",
      "Epoch 150, CIFAR-10 Batch 2:  Loss:     0.5803 Validation Accuracy: 0.626400\n",
      "Epoch 150, CIFAR-10 Batch 3:  Loss:     0.4289 Validation Accuracy: 0.624200\n",
      "Epoch 150, CIFAR-10 Batch 4:  Loss:     0.5049 Validation Accuracy: 0.623000\n",
      "Epoch 150, CIFAR-10 Batch 5:  Loss:     0.4904 Validation Accuracy: 0.618200\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss:     0.6148 Validation Accuracy: 0.622200\n",
      "Epoch 151, CIFAR-10 Batch 2:  Loss:     0.5954 Validation Accuracy: 0.627400\n",
      "Epoch 151, CIFAR-10 Batch 3:  Loss:     0.4390 Validation Accuracy: 0.623000\n",
      "Epoch 151, CIFAR-10 Batch 4:  Loss:     0.5099 Validation Accuracy: 0.621400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151, CIFAR-10 Batch 5:  Loss:     0.4703 Validation Accuracy: 0.622200\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss:     0.6076 Validation Accuracy: 0.619800\n",
      "Epoch 152, CIFAR-10 Batch 2:  Loss:     0.5862 Validation Accuracy: 0.626800\n",
      "Epoch 152, CIFAR-10 Batch 3:  Loss:     0.4262 Validation Accuracy: 0.625400\n",
      "Epoch 152, CIFAR-10 Batch 4:  Loss:     0.4881 Validation Accuracy: 0.622200\n",
      "Epoch 152, CIFAR-10 Batch 5:  Loss:     0.4813 Validation Accuracy: 0.616400\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss:     0.6141 Validation Accuracy: 0.621600\n",
      "Epoch 153, CIFAR-10 Batch 2:  Loss:     0.5814 Validation Accuracy: 0.621000\n",
      "Epoch 153, CIFAR-10 Batch 3:  Loss:     0.4262 Validation Accuracy: 0.624800\n",
      "Epoch 153, CIFAR-10 Batch 4:  Loss:     0.5013 Validation Accuracy: 0.618800\n",
      "Epoch 153, CIFAR-10 Batch 5:  Loss:     0.4800 Validation Accuracy: 0.618200\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss:     0.6197 Validation Accuracy: 0.621600\n",
      "Epoch 154, CIFAR-10 Batch 2:  Loss:     0.5671 Validation Accuracy: 0.626600\n",
      "Epoch 154, CIFAR-10 Batch 3:  Loss:     0.4225 Validation Accuracy: 0.625400\n",
      "Epoch 154, CIFAR-10 Batch 4:  Loss:     0.4958 Validation Accuracy: 0.623400\n",
      "Epoch 154, CIFAR-10 Batch 5:  Loss:     0.4757 Validation Accuracy: 0.617400\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss:     0.6106 Validation Accuracy: 0.623400\n",
      "Epoch 155, CIFAR-10 Batch 2:  Loss:     0.5653 Validation Accuracy: 0.625000\n",
      "Epoch 155, CIFAR-10 Batch 3:  Loss:     0.4233 Validation Accuracy: 0.622000\n",
      "Epoch 155, CIFAR-10 Batch 4:  Loss:     0.4907 Validation Accuracy: 0.625200\n",
      "Epoch 155, CIFAR-10 Batch 5:  Loss:     0.4845 Validation Accuracy: 0.623000\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss:     0.6006 Validation Accuracy: 0.623200\n",
      "Epoch 156, CIFAR-10 Batch 2:  Loss:     0.5603 Validation Accuracy: 0.627800\n",
      "Epoch 156, CIFAR-10 Batch 3:  Loss:     0.4254 Validation Accuracy: 0.625600\n",
      "Epoch 156, CIFAR-10 Batch 4:  Loss:     0.4923 Validation Accuracy: 0.620400\n",
      "Epoch 156, CIFAR-10 Batch 5:  Loss:     0.4796 Validation Accuracy: 0.621800\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss:     0.6225 Validation Accuracy: 0.623800\n",
      "Epoch 157, CIFAR-10 Batch 2:  Loss:     0.5675 Validation Accuracy: 0.625800\n",
      "Epoch 157, CIFAR-10 Batch 3:  Loss:     0.4204 Validation Accuracy: 0.624400\n",
      "Epoch 157, CIFAR-10 Batch 4:  Loss:     0.4917 Validation Accuracy: 0.623600\n",
      "Epoch 157, CIFAR-10 Batch 5:  Loss:     0.4819 Validation Accuracy: 0.623800\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss:     0.6114 Validation Accuracy: 0.623000\n",
      "Epoch 158, CIFAR-10 Batch 2:  Loss:     0.5773 Validation Accuracy: 0.630600\n",
      "Epoch 158, CIFAR-10 Batch 3:  Loss:     0.4279 Validation Accuracy: 0.621800\n",
      "Epoch 158, CIFAR-10 Batch 4:  Loss:     0.4992 Validation Accuracy: 0.624400\n",
      "Epoch 158, CIFAR-10 Batch 5:  Loss:     0.4906 Validation Accuracy: 0.621200\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss:     0.6166 Validation Accuracy: 0.626000\n",
      "Epoch 159, CIFAR-10 Batch 2:  Loss:     0.5722 Validation Accuracy: 0.627800\n",
      "Epoch 159, CIFAR-10 Batch 3:  Loss:     0.4212 Validation Accuracy: 0.627000\n",
      "Epoch 159, CIFAR-10 Batch 4:  Loss:     0.5041 Validation Accuracy: 0.620800\n",
      "Epoch 159, CIFAR-10 Batch 5:  Loss:     0.4829 Validation Accuracy: 0.617800\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss:     0.6223 Validation Accuracy: 0.624800\n",
      "Epoch 160, CIFAR-10 Batch 2:  Loss:     0.5650 Validation Accuracy: 0.628200\n",
      "Epoch 160, CIFAR-10 Batch 3:  Loss:     0.4293 Validation Accuracy: 0.624400\n",
      "Epoch 160, CIFAR-10 Batch 4:  Loss:     0.5049 Validation Accuracy: 0.622400\n",
      "Epoch 160, CIFAR-10 Batch 5:  Loss:     0.4835 Validation Accuracy: 0.619600\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss:     0.6329 Validation Accuracy: 0.623000\n",
      "Epoch 161, CIFAR-10 Batch 2:  Loss:     0.5684 Validation Accuracy: 0.627200\n",
      "Epoch 161, CIFAR-10 Batch 3:  Loss:     0.4314 Validation Accuracy: 0.622000\n",
      "Epoch 161, CIFAR-10 Batch 4:  Loss:     0.4965 Validation Accuracy: 0.625600\n",
      "Epoch 161, CIFAR-10 Batch 5:  Loss:     0.4759 Validation Accuracy: 0.624000\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss:     0.6066 Validation Accuracy: 0.626400\n",
      "Epoch 162, CIFAR-10 Batch 2:  Loss:     0.5789 Validation Accuracy: 0.628800\n",
      "Epoch 162, CIFAR-10 Batch 3:  Loss:     0.4281 Validation Accuracy: 0.623800\n",
      "Epoch 162, CIFAR-10 Batch 4:  Loss:     0.5043 Validation Accuracy: 0.620200\n",
      "Epoch 162, CIFAR-10 Batch 5:  Loss:     0.4768 Validation Accuracy: 0.619400\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss:     0.6166 Validation Accuracy: 0.623200\n",
      "Epoch 163, CIFAR-10 Batch 2:  Loss:     0.5662 Validation Accuracy: 0.627400\n",
      "Epoch 163, CIFAR-10 Batch 3:  Loss:     0.4320 Validation Accuracy: 0.625800\n",
      "Epoch 163, CIFAR-10 Batch 4:  Loss:     0.4979 Validation Accuracy: 0.624400\n",
      "Epoch 163, CIFAR-10 Batch 5:  Loss:     0.4722 Validation Accuracy: 0.619600\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss:     0.6189 Validation Accuracy: 0.623400\n",
      "Epoch 164, CIFAR-10 Batch 2:  Loss:     0.5609 Validation Accuracy: 0.629800\n",
      "Epoch 164, CIFAR-10 Batch 3:  Loss:     0.4245 Validation Accuracy: 0.626600\n",
      "Epoch 164, CIFAR-10 Batch 4:  Loss:     0.4992 Validation Accuracy: 0.628800\n",
      "Epoch 164, CIFAR-10 Batch 5:  Loss:     0.4634 Validation Accuracy: 0.619600\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss:     0.6027 Validation Accuracy: 0.622600\n",
      "Epoch 165, CIFAR-10 Batch 2:  Loss:     0.5703 Validation Accuracy: 0.629600\n",
      "Epoch 165, CIFAR-10 Batch 3:  Loss:     0.4221 Validation Accuracy: 0.623800\n",
      "Epoch 165, CIFAR-10 Batch 4:  Loss:     0.4891 Validation Accuracy: 0.625400\n",
      "Epoch 165, CIFAR-10 Batch 5:  Loss:     0.4552 Validation Accuracy: 0.619000\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss:     0.6048 Validation Accuracy: 0.623400\n",
      "Epoch 166, CIFAR-10 Batch 2:  Loss:     0.5683 Validation Accuracy: 0.624800\n",
      "Epoch 166, CIFAR-10 Batch 3:  Loss:     0.4081 Validation Accuracy: 0.627200\n",
      "Epoch 166, CIFAR-10 Batch 4:  Loss:     0.4838 Validation Accuracy: 0.624200\n",
      "Epoch 166, CIFAR-10 Batch 5:  Loss:     0.4578 Validation Accuracy: 0.617400\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss:     0.6122 Validation Accuracy: 0.626200\n",
      "Epoch 167, CIFAR-10 Batch 2:  Loss:     0.5724 Validation Accuracy: 0.627600\n",
      "Epoch 167, CIFAR-10 Batch 3:  Loss:     0.4075 Validation Accuracy: 0.626000\n",
      "Epoch 167, CIFAR-10 Batch 4:  Loss:     0.5076 Validation Accuracy: 0.627400\n",
      "Epoch 167, CIFAR-10 Batch 5:  Loss:     0.4662 Validation Accuracy: 0.625200\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss:     0.6031 Validation Accuracy: 0.625600\n",
      "Epoch 168, CIFAR-10 Batch 2:  Loss:     0.5532 Validation Accuracy: 0.627800\n",
      "Epoch 168, CIFAR-10 Batch 3:  Loss:     0.4230 Validation Accuracy: 0.626800\n",
      "Epoch 168, CIFAR-10 Batch 4:  Loss:     0.5033 Validation Accuracy: 0.627000\n",
      "Epoch 168, CIFAR-10 Batch 5:  Loss:     0.4664 Validation Accuracy: 0.622200\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss:     0.6048 Validation Accuracy: 0.626000\n",
      "Epoch 169, CIFAR-10 Batch 2:  Loss:     0.5635 Validation Accuracy: 0.628800\n",
      "Epoch 169, CIFAR-10 Batch 3:  Loss:     0.4141 Validation Accuracy: 0.627600\n",
      "Epoch 169, CIFAR-10 Batch 4:  Loss:     0.5093 Validation Accuracy: 0.625000\n",
      "Epoch 169, CIFAR-10 Batch 5:  Loss:     0.4753 Validation Accuracy: 0.620800\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss:     0.6148 Validation Accuracy: 0.621600\n",
      "Epoch 170, CIFAR-10 Batch 2:  Loss:     0.5590 Validation Accuracy: 0.631400\n",
      "Epoch 170, CIFAR-10 Batch 3:  Loss:     0.4340 Validation Accuracy: 0.623400\n",
      "Epoch 170, CIFAR-10 Batch 4:  Loss:     0.4925 Validation Accuracy: 0.624400\n",
      "Epoch 170, CIFAR-10 Batch 5:  Loss:     0.4661 Validation Accuracy: 0.623800\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss:     0.5958 Validation Accuracy: 0.627200\n",
      "Epoch 171, CIFAR-10 Batch 2:  Loss:     0.5440 Validation Accuracy: 0.626400\n",
      "Epoch 171, CIFAR-10 Batch 3:  Loss:     0.4210 Validation Accuracy: 0.624000\n",
      "Epoch 171, CIFAR-10 Batch 4:  Loss:     0.4953 Validation Accuracy: 0.621800\n",
      "Epoch 171, CIFAR-10 Batch 5:  Loss:     0.4713 Validation Accuracy: 0.625800\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss:     0.6053 Validation Accuracy: 0.624600\n",
      "Epoch 172, CIFAR-10 Batch 2:  Loss:     0.5531 Validation Accuracy: 0.625400\n",
      "Epoch 172, CIFAR-10 Batch 3:  Loss:     0.4102 Validation Accuracy: 0.625000\n",
      "Epoch 172, CIFAR-10 Batch 4:  Loss:     0.4974 Validation Accuracy: 0.624000\n",
      "Epoch 172, CIFAR-10 Batch 5:  Loss:     0.4669 Validation Accuracy: 0.623000\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss:     0.5872 Validation Accuracy: 0.624800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173, CIFAR-10 Batch 2:  Loss:     0.5520 Validation Accuracy: 0.630000\n",
      "Epoch 173, CIFAR-10 Batch 3:  Loss:     0.4181 Validation Accuracy: 0.628400\n",
      "Epoch 173, CIFAR-10 Batch 4:  Loss:     0.5045 Validation Accuracy: 0.627200\n",
      "Epoch 173, CIFAR-10 Batch 5:  Loss:     0.4641 Validation Accuracy: 0.623400\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss:     0.6135 Validation Accuracy: 0.626200\n",
      "Epoch 174, CIFAR-10 Batch 2:  Loss:     0.5538 Validation Accuracy: 0.628800\n",
      "Epoch 174, CIFAR-10 Batch 3:  Loss:     0.4213 Validation Accuracy: 0.625800\n",
      "Epoch 174, CIFAR-10 Batch 4:  Loss:     0.4931 Validation Accuracy: 0.622400\n",
      "Epoch 174, CIFAR-10 Batch 5:  Loss:     0.4726 Validation Accuracy: 0.621800\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss:     0.5929 Validation Accuracy: 0.625800\n",
      "Epoch 175, CIFAR-10 Batch 2:  Loss:     0.5434 Validation Accuracy: 0.627800\n",
      "Epoch 175, CIFAR-10 Batch 3:  Loss:     0.4226 Validation Accuracy: 0.625000\n",
      "Epoch 175, CIFAR-10 Batch 4:  Loss:     0.4957 Validation Accuracy: 0.625200\n",
      "Epoch 175, CIFAR-10 Batch 5:  Loss:     0.4622 Validation Accuracy: 0.622400\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss:     0.5998 Validation Accuracy: 0.628400\n",
      "Epoch 176, CIFAR-10 Batch 2:  Loss:     0.5606 Validation Accuracy: 0.630800\n",
      "Epoch 176, CIFAR-10 Batch 3:  Loss:     0.4146 Validation Accuracy: 0.627800\n",
      "Epoch 176, CIFAR-10 Batch 4:  Loss:     0.4921 Validation Accuracy: 0.622400\n",
      "Epoch 176, CIFAR-10 Batch 5:  Loss:     0.4738 Validation Accuracy: 0.619800\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss:     0.5885 Validation Accuracy: 0.623600\n",
      "Epoch 177, CIFAR-10 Batch 2:  Loss:     0.5483 Validation Accuracy: 0.626800\n",
      "Epoch 177, CIFAR-10 Batch 3:  Loss:     0.4228 Validation Accuracy: 0.623000\n",
      "Epoch 177, CIFAR-10 Batch 4:  Loss:     0.4821 Validation Accuracy: 0.623400\n",
      "Epoch 177, CIFAR-10 Batch 5:  Loss:     0.4669 Validation Accuracy: 0.623800\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss:     0.5819 Validation Accuracy: 0.623000\n",
      "Epoch 178, CIFAR-10 Batch 2:  Loss:     0.5424 Validation Accuracy: 0.631400\n",
      "Epoch 178, CIFAR-10 Batch 3:  Loss:     0.4085 Validation Accuracy: 0.626200\n",
      "Epoch 178, CIFAR-10 Batch 4:  Loss:     0.4800 Validation Accuracy: 0.624200\n",
      "Epoch 178, CIFAR-10 Batch 5:  Loss:     0.4674 Validation Accuracy: 0.625600\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss:     0.6084 Validation Accuracy: 0.622200\n",
      "Epoch 179, CIFAR-10 Batch 2:  Loss:     0.5537 Validation Accuracy: 0.629600\n",
      "Epoch 179, CIFAR-10 Batch 3:  Loss:     0.4199 Validation Accuracy: 0.626000\n",
      "Epoch 179, CIFAR-10 Batch 4:  Loss:     0.4691 Validation Accuracy: 0.627200\n",
      "Epoch 179, CIFAR-10 Batch 5:  Loss:     0.4620 Validation Accuracy: 0.622000\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss:     0.5864 Validation Accuracy: 0.622000\n",
      "Epoch 180, CIFAR-10 Batch 2:  Loss:     0.5418 Validation Accuracy: 0.629000\n",
      "Epoch 180, CIFAR-10 Batch 3:  Loss:     0.4054 Validation Accuracy: 0.626000\n",
      "Epoch 180, CIFAR-10 Batch 4:  Loss:     0.4760 Validation Accuracy: 0.622200\n",
      "Epoch 180, CIFAR-10 Batch 5:  Loss:     0.4699 Validation Accuracy: 0.621800\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss:     0.5797 Validation Accuracy: 0.624000\n",
      "Epoch 181, CIFAR-10 Batch 2:  Loss:     0.5371 Validation Accuracy: 0.629200\n",
      "Epoch 181, CIFAR-10 Batch 3:  Loss:     0.4171 Validation Accuracy: 0.626600\n",
      "Epoch 181, CIFAR-10 Batch 4:  Loss:     0.4769 Validation Accuracy: 0.626600\n",
      "Epoch 181, CIFAR-10 Batch 5:  Loss:     0.4593 Validation Accuracy: 0.622200\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss:     0.5830 Validation Accuracy: 0.624800\n",
      "Epoch 182, CIFAR-10 Batch 2:  Loss:     0.5556 Validation Accuracy: 0.629800\n",
      "Epoch 182, CIFAR-10 Batch 3:  Loss:     0.4185 Validation Accuracy: 0.624400\n",
      "Epoch 182, CIFAR-10 Batch 4:  Loss:     0.4870 Validation Accuracy: 0.625400\n",
      "Epoch 182, CIFAR-10 Batch 5:  Loss:     0.4708 Validation Accuracy: 0.621000\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss:     0.5915 Validation Accuracy: 0.628800\n",
      "Epoch 183, CIFAR-10 Batch 2:  Loss:     0.5335 Validation Accuracy: 0.630800\n",
      "Epoch 183, CIFAR-10 Batch 3:  Loss:     0.4150 Validation Accuracy: 0.620800\n",
      "Epoch 183, CIFAR-10 Batch 4:  Loss:     0.4819 Validation Accuracy: 0.620000\n",
      "Epoch 183, CIFAR-10 Batch 5:  Loss:     0.4595 Validation Accuracy: 0.620200\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss:     0.5889 Validation Accuracy: 0.622600\n",
      "Epoch 184, CIFAR-10 Batch 2:  Loss:     0.5344 Validation Accuracy: 0.631200\n",
      "Epoch 184, CIFAR-10 Batch 3:  Loss:     0.4198 Validation Accuracy: 0.626800\n",
      "Epoch 184, CIFAR-10 Batch 4:  Loss:     0.4827 Validation Accuracy: 0.628400\n",
      "Epoch 184, CIFAR-10 Batch 5:  Loss:     0.4623 Validation Accuracy: 0.625400\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss:     0.5826 Validation Accuracy: 0.627000\n",
      "Epoch 185, CIFAR-10 Batch 2:  Loss:     0.5368 Validation Accuracy: 0.631600\n",
      "Epoch 185, CIFAR-10 Batch 3:  Loss:     0.4071 Validation Accuracy: 0.624000\n",
      "Epoch 185, CIFAR-10 Batch 4:  Loss:     0.4819 Validation Accuracy: 0.625400\n",
      "Epoch 185, CIFAR-10 Batch 5:  Loss:     0.4584 Validation Accuracy: 0.625600\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss:     0.5868 Validation Accuracy: 0.625800\n",
      "Epoch 186, CIFAR-10 Batch 2:  Loss:     0.5394 Validation Accuracy: 0.628000\n",
      "Epoch 186, CIFAR-10 Batch 3:  Loss:     0.4214 Validation Accuracy: 0.626400\n",
      "Epoch 186, CIFAR-10 Batch 4:  Loss:     0.4723 Validation Accuracy: 0.623800\n",
      "Epoch 186, CIFAR-10 Batch 5:  Loss:     0.4568 Validation Accuracy: 0.623200\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss:     0.5804 Validation Accuracy: 0.627000\n",
      "Epoch 187, CIFAR-10 Batch 2:  Loss:     0.5202 Validation Accuracy: 0.627800\n",
      "Epoch 187, CIFAR-10 Batch 3:  Loss:     0.4077 Validation Accuracy: 0.625000\n",
      "Epoch 187, CIFAR-10 Batch 4:  Loss:     0.4834 Validation Accuracy: 0.625600\n",
      "Epoch 187, CIFAR-10 Batch 5:  Loss:     0.4714 Validation Accuracy: 0.623600\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss:     0.5893 Validation Accuracy: 0.626000\n",
      "Epoch 188, CIFAR-10 Batch 2:  Loss:     0.5437 Validation Accuracy: 0.630400\n",
      "Epoch 188, CIFAR-10 Batch 3:  Loss:     0.4117 Validation Accuracy: 0.626800\n",
      "Epoch 188, CIFAR-10 Batch 4:  Loss:     0.4805 Validation Accuracy: 0.624200\n",
      "Epoch 188, CIFAR-10 Batch 5:  Loss:     0.4513 Validation Accuracy: 0.628000\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss:     0.5846 Validation Accuracy: 0.622800\n",
      "Epoch 189, CIFAR-10 Batch 2:  Loss:     0.5244 Validation Accuracy: 0.631400\n",
      "Epoch 189, CIFAR-10 Batch 3:  Loss:     0.4161 Validation Accuracy: 0.623400\n",
      "Epoch 189, CIFAR-10 Batch 4:  Loss:     0.4729 Validation Accuracy: 0.624000\n",
      "Epoch 189, CIFAR-10 Batch 5:  Loss:     0.4434 Validation Accuracy: 0.623200\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss:     0.5899 Validation Accuracy: 0.626800\n",
      "Epoch 190, CIFAR-10 Batch 2:  Loss:     0.5505 Validation Accuracy: 0.627400\n",
      "Epoch 190, CIFAR-10 Batch 3:  Loss:     0.4118 Validation Accuracy: 0.627200\n",
      "Epoch 190, CIFAR-10 Batch 4:  Loss:     0.4716 Validation Accuracy: 0.630000\n",
      "Epoch 190, CIFAR-10 Batch 5:  Loss:     0.4627 Validation Accuracy: 0.627000\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss:     0.6083 Validation Accuracy: 0.627200\n",
      "Epoch 191, CIFAR-10 Batch 2:  Loss:     0.5386 Validation Accuracy: 0.629800\n",
      "Epoch 191, CIFAR-10 Batch 3:  Loss:     0.4160 Validation Accuracy: 0.626400\n",
      "Epoch 191, CIFAR-10 Batch 4:  Loss:     0.4628 Validation Accuracy: 0.629000\n",
      "Epoch 191, CIFAR-10 Batch 5:  Loss:     0.4492 Validation Accuracy: 0.623200\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss:     0.5876 Validation Accuracy: 0.624800\n",
      "Epoch 192, CIFAR-10 Batch 2:  Loss:     0.5362 Validation Accuracy: 0.633000\n",
      "Epoch 192, CIFAR-10 Batch 3:  Loss:     0.4121 Validation Accuracy: 0.627800\n",
      "Epoch 192, CIFAR-10 Batch 4:  Loss:     0.4649 Validation Accuracy: 0.628400\n",
      "Epoch 192, CIFAR-10 Batch 5:  Loss:     0.4540 Validation Accuracy: 0.626400\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss:     0.6003 Validation Accuracy: 0.624000\n",
      "Epoch 193, CIFAR-10 Batch 2:  Loss:     0.5286 Validation Accuracy: 0.631600\n",
      "Epoch 193, CIFAR-10 Batch 3:  Loss:     0.4063 Validation Accuracy: 0.626400\n",
      "Epoch 193, CIFAR-10 Batch 4:  Loss:     0.4845 Validation Accuracy: 0.623600\n",
      "Epoch 193, CIFAR-10 Batch 5:  Loss:     0.4579 Validation Accuracy: 0.623800\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss:     0.5908 Validation Accuracy: 0.625800\n",
      "Epoch 194, CIFAR-10 Batch 2:  Loss:     0.5319 Validation Accuracy: 0.628800\n",
      "Epoch 194, CIFAR-10 Batch 3:  Loss:     0.4102 Validation Accuracy: 0.622200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194, CIFAR-10 Batch 4:  Loss:     0.4728 Validation Accuracy: 0.625800\n",
      "Epoch 194, CIFAR-10 Batch 5:  Loss:     0.4488 Validation Accuracy: 0.625600\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss:     0.5850 Validation Accuracy: 0.625800\n",
      "Epoch 195, CIFAR-10 Batch 2:  Loss:     0.5250 Validation Accuracy: 0.629000\n",
      "Epoch 195, CIFAR-10 Batch 3:  Loss:     0.4134 Validation Accuracy: 0.625000\n",
      "Epoch 195, CIFAR-10 Batch 4:  Loss:     0.4828 Validation Accuracy: 0.627200\n",
      "Epoch 195, CIFAR-10 Batch 5:  Loss:     0.4480 Validation Accuracy: 0.624800\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss:     0.5817 Validation Accuracy: 0.624800\n",
      "Epoch 196, CIFAR-10 Batch 2:  Loss:     0.5289 Validation Accuracy: 0.626400\n",
      "Epoch 196, CIFAR-10 Batch 3:  Loss:     0.4160 Validation Accuracy: 0.619800\n",
      "Epoch 196, CIFAR-10 Batch 4:  Loss:     0.4834 Validation Accuracy: 0.627000\n",
      "Epoch 196, CIFAR-10 Batch 5:  Loss:     0.4373 Validation Accuracy: 0.621600\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss:     0.5896 Validation Accuracy: 0.627200\n",
      "Epoch 197, CIFAR-10 Batch 2:  Loss:     0.5391 Validation Accuracy: 0.630800\n",
      "Epoch 197, CIFAR-10 Batch 3:  Loss:     0.4169 Validation Accuracy: 0.626000\n",
      "Epoch 197, CIFAR-10 Batch 4:  Loss:     0.4791 Validation Accuracy: 0.625400\n",
      "Epoch 197, CIFAR-10 Batch 5:  Loss:     0.4435 Validation Accuracy: 0.623400\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss:     0.6082 Validation Accuracy: 0.630200\n",
      "Epoch 198, CIFAR-10 Batch 2:  Loss:     0.5225 Validation Accuracy: 0.631800\n",
      "Epoch 198, CIFAR-10 Batch 3:  Loss:     0.4092 Validation Accuracy: 0.627800\n",
      "Epoch 198, CIFAR-10 Batch 4:  Loss:     0.4856 Validation Accuracy: 0.628400\n",
      "Epoch 198, CIFAR-10 Batch 5:  Loss:     0.4456 Validation Accuracy: 0.626000\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss:     0.5677 Validation Accuracy: 0.626800\n",
      "Epoch 199, CIFAR-10 Batch 2:  Loss:     0.5271 Validation Accuracy: 0.629000\n",
      "Epoch 199, CIFAR-10 Batch 3:  Loss:     0.4050 Validation Accuracy: 0.626200\n",
      "Epoch 199, CIFAR-10 Batch 4:  Loss:     0.4747 Validation Accuracy: 0.628400\n",
      "Epoch 199, CIFAR-10 Batch 5:  Loss:     0.4522 Validation Accuracy: 0.627000\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss:     0.6137 Validation Accuracy: 0.628400\n",
      "Epoch 200, CIFAR-10 Batch 2:  Loss:     0.5256 Validation Accuracy: 0.632200\n",
      "Epoch 200, CIFAR-10 Batch 3:  Loss:     0.4111 Validation Accuracy: 0.626200\n",
      "Epoch 200, CIFAR-10 Batch 4:  Loss:     0.4796 Validation Accuracy: 0.626600\n",
      "Epoch 200, CIFAR-10 Batch 5:  Loss:     0.4571 Validation Accuracy: 0.622200\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/output\n",
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.63046875\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa143623a20>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
